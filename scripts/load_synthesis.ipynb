{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script implements GAN for Load Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..', 'demand_acep'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = os.path.join(module_path, 'data/measurements/test_data')\n",
    "filename = 'PQube3_comb.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_label(meter_df, data_shape):\n",
    "    meter_data = meter_df.values\n",
    "    data_mean = np.mean(meter_data)\n",
    "    data_std = np.std(meter_data)\n",
    "    # limits = [data_mean - (data_std / 2), data_mean + (data_std / 2)]\n",
    "    limits = [data_mean - data_std, data_mean + data_std]\n",
    "    data_size = len(meter_data) // data_shape\n",
    "    # pdb.set_trace()\n",
    "    data_select = meter_data[:data_size * data_shape]\n",
    "    data_x = data_select.reshape([data_size, data_shape])\n",
    "    data_y_label = np.mean(data_x, axis=1)\n",
    "    data_y = np.empty((data_size, 1))\n",
    "    data_y[data_y_label < limits[0]] = 0\n",
    "    data_y[(data_y_label >= limits[0]) & (data_y_label <= limits[1])] = 1\n",
    "    data_y[data_y_label > limits[1]] = 2\n",
    "    # data_y = np.select([data_y_label < limits[0], (data_y_label >= limits[0]) & (data_y_label <= limits[1]),\n",
    "    #                     data_y_label > limits[1]], [0, 1, 2])\n",
    "    # data_y = data_y.reshape(data_y, (len(data_y), 1))\n",
    "\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation constants\n",
    "num_epochs = 70\n",
    "learning_rate = 1e-4\n",
    "data_shape = 1440\n",
    "batch_size = 200\n",
    "noise_size = 100\n",
    "mu, sigma = 0, 0.001 # input Gaussian\n",
    "path_data = '/Users/Tinu/Dropbox/demand_acep/demand_acep/data/measurements/test_data/filled_sample'\n",
    "\n",
    "meter_df = pd.DataFrame()\n",
    "for dirpath, dirnames, files in os.walk(path_data, topdown=True):\n",
    "    for filename in files:\n",
    "        if filename.lower().startswith('pqube'):\n",
    "        # if filename.lower().endswith('.csv'):\n",
    "            read_meter = pd.read_csv(os.path.join(dirpath, filename), header=None, usecols=[1])\n",
    "            if meter_df.empty:\n",
    "                meter_df = read_meter.copy()\n",
    "            else:\n",
    "                meter_df = meter_df.append(read_meter, sort=False)\n",
    "\n",
    "# Load data\n",
    "data_x_us, data_y = load_data_label(meter_df, data_shape)\n",
    "scaler_x = MinMaxScaler()\n",
    "data_x = scaler_x.fit_transform(data_x_us)\n",
    "# train_data = TensorDataset(torch.from_numpy(data_x), torch.from_numpy(data_y))\n",
    "# data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# train_data = TensorDataset(torch.from_numpy(data_x))\n",
    "data_loader = DataLoader(torch.from_numpy(data_x), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables and Main functions\n",
    "dim_G_1 = noise_size\n",
    "dim_G_2 = 2000\n",
    "dim_G_3 = 7000\n",
    "dim_G_4 = 5000\n",
    "dim_G_out = data_shape\n",
    "\n",
    "dim_D_1 = data_shape\n",
    "dim_D_2 = 10000\n",
    "dim_D_3 = 7000\n",
    "dim_D_4 = 1440\n",
    "dim_D_out = 1\n",
    "\n",
    "\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Tinu/miniconda3/envs/acep/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Graph Initialization\n",
    "weight_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "bias_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "\n",
    "# Generator\n",
    "# Input\n",
    "Z = tf.placeholder(tf.float32, shape=(None, noise_size), name='Z')\n",
    "G_W1 = tf.get_variable(name='G_Weight_1', dtype=tf.float32, shape=[dim_G_1, dim_G_2], initializer=weight_init)\n",
    "G_W2 = tf.get_variable(name='G_Weight_2', dtype=tf.float32, shape=[dim_G_2, dim_G_3], initializer=weight_init)\n",
    "G_W3 = tf.get_variable(name='G_Weight_3', dtype=tf.float32, shape=[dim_G_3, dim_G_4], initializer=weight_init)\n",
    "G_W4 = tf.get_variable(name='G_Weight_4', dtype=tf.float32, shape=[dim_G_4, dim_G_out], initializer=weight_init)\n",
    "\n",
    "\n",
    "G_B1 = tf.get_variable(name='G_bias_1', dtype=tf.float32, shape=[dim_G_2], initializer=bias_init)\n",
    "G_B2 = tf.get_variable(name='G_bias_2', dtype=tf.float32, shape=[dim_G_3], initializer=bias_init)\n",
    "G_B3 = tf.get_variable(name='G_bias_3', dtype=tf.float32, shape=[dim_G_4], initializer=bias_init)\n",
    "G_B4 = tf.get_variable(name='G_bias_4', dtype=tf.float32, shape=[dim_G_out], initializer=bias_init)\n",
    "\n",
    "gen_vars = [G_W1, G_B1, G_W2, G_B2, G_W3, G_B3, G_W4, G_B4]\n",
    "\n",
    "# Discriminator\n",
    "# Input\n",
    "X = tf.placeholder(tf.float32, shape=(None, data_shape), name='X')\n",
    "\n",
    "D_W1 = tf.get_variable(name='D_Weight_1', dtype=tf.float32, shape=[dim_D_1, dim_D_2], initializer=weight_init)\n",
    "D_W2 = tf.get_variable(name='D_Weight_2', dtype=tf.float32, shape=[dim_D_2, dim_D_3], initializer=weight_init)\n",
    "D_W3 = tf.get_variable(name='D_Weight_3', dtype=tf.float32, shape=[dim_D_3, dim_D_4], initializer=weight_init)\n",
    "D_W4 = tf.get_variable(name='D_Weight_4', dtype=tf.float32, shape=[dim_D_4, dim_D_out], initializer=weight_init)\n",
    "\n",
    "\n",
    "D_B1 = tf.get_variable(name='D_bias_1', dtype=tf.float32, shape=[dim_D_2], initializer=bias_init)\n",
    "D_B2 = tf.get_variable(name='D_bias_2', dtype=tf.float32, shape=[dim_D_3], initializer=bias_init)\n",
    "D_B3 = tf.get_variable(name='D_bias_3', dtype=tf.float32, shape=[dim_D_4], initializer=bias_init)\n",
    "D_B4 = tf.get_variable(name='D_bias_4', dtype=tf.float32, shape=[dim_D_out], initializer=bias_init)\n",
    "\n",
    "dis_vars = [D_W1, D_B1, D_W2, D_B2, D_W3, D_B3, D_W4, D_B4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator Function Definition\n",
    "def generator(z):\n",
    "    l1 = tf.nn.leaky_relu(tf.matmul(z,  G_W1) + G_B1, .2)\n",
    "    l2 = tf.nn.leaky_relu(tf.matmul(l1, G_W2) + G_B2, .2)\n",
    "    l3 = tf.nn.leaky_relu(tf.matmul(l2, G_W3) + G_B3, .2)\n",
    "    out = tf.nn.tanh(tf.matmul(l3, G_W4) + G_B4)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def discriminator(x):\n",
    "    l1 = tf.nn.leaky_relu(tf.matmul(x,   D_W1) + D_B1, .2)\n",
    "    l2 = tf.nn.leaky_relu(tf.matmul(l1,  D_W2) + D_B2, .2)\n",
    "    l3 = tf.nn.leaky_relu(tf.matmul(l2,  D_W3) + D_B3, .2)\n",
    "    out_logit = tf.matmul(l3, D_W4) + D_B4\n",
    "    out = tf.nn.sigmoid(out_logit)\n",
    "\n",
    "    return out, out_logit\n",
    "\n",
    "\n",
    "def noise(n_rows, n_cols):\n",
    "    # return np.random.normal(mu, sigma, size=(n_rows, n_cols))\n",
    "    return np.random.uniform(-1., 1., size=(n_rows, n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers and losses\n",
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# Losses\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real, labels=tf.ones_like(D_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake, labels=tf.zeros_like(D_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake, labels=tf.ones_like(D_fake)))\n",
    "\n",
    "# Optimizers\n",
    "D_opt = tf.train.AdamOptimizer(learning_rate).minimize(D_loss, var_list=dis_vars)\n",
    "G_opt = tf.train.AdamOptimizer(learning_rate).minimize(G_loss, var_list=gen_vars)\n",
    "\n",
    "# Start interactive session\n",
    "session = tf.InteractiveSession()\n",
    "# Init Variables\n",
    "tf.global_variables_initializer().run(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0\t Discriminator loss: 1.4476\t Generator loss: 0.5058\n",
      "Iterations: 1\t Discriminator loss: 1.4997\t Generator loss: 0.4311\n",
      "Iterations: 2\t Discriminator loss: 1.4482\t Generator loss: 0.6241\n",
      "Iterations: 3\t Discriminator loss: 1.3669\t Generator loss: 0.6916\n",
      "Iterations: 4\t Discriminator loss: 1.3716\t Generator loss: 0.6931\n",
      "Iterations: 5\t Discriminator loss: 1.3746\t Generator loss: 0.6931\n",
      "Iterations: 6\t Discriminator loss: 1.3710\t Generator loss: 0.6931\n",
      "Iterations: 7\t Discriminator loss: 1.3674\t Generator loss: 0.6931\n",
      "Iterations: 8\t Discriminator loss: 1.3464\t Generator loss: 0.6931\n",
      "Iterations: 9\t Discriminator loss: 1.0664\t Generator loss: 0.6910\n",
      "Iterations: 10\t Discriminator loss: 1.0305\t Generator loss: 0.6931\n",
      "Iterations: 11\t Discriminator loss: 1.0181\t Generator loss: 0.6931\n",
      "Iterations: 12\t Discriminator loss: 1.0138\t Generator loss: 0.6931\n",
      "Iterations: 13\t Discriminator loss: 1.0104\t Generator loss: 0.6931\n",
      "Iterations: 14\t Discriminator loss: 1.0098\t Generator loss: 0.6931\n",
      "Iterations: 15\t Discriminator loss: 1.0082\t Generator loss: 0.6931\n",
      "Iterations: 16\t Discriminator loss: 1.0074\t Generator loss: 0.6931\n",
      "Iterations: 17\t Discriminator loss: 1.0076\t Generator loss: 0.6931\n",
      "Iterations: 18\t Discriminator loss: 1.0072\t Generator loss: 0.6931\n",
      "Iterations: 19\t Discriminator loss: 1.0069\t Generator loss: 0.6931\n",
      "Iterations: 20\t Discriminator loss: 1.0068\t Generator loss: 0.6931\n",
      "Iterations: 21\t Discriminator loss: 1.0068\t Generator loss: 0.6931\n",
      "Iterations: 22\t Discriminator loss: 1.0067\t Generator loss: 0.6931\n",
      "Iterations: 23\t Discriminator loss: 1.0066\t Generator loss: 0.6931\n",
      "Iterations: 24\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 25\t Discriminator loss: 1.0066\t Generator loss: 0.6931\n",
      "Iterations: 26\t Discriminator loss: 1.0066\t Generator loss: 0.6931\n",
      "Iterations: 27\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 28\t Discriminator loss: 1.0066\t Generator loss: 0.6931\n",
      "Iterations: 29\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 30\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 31\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 32\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 33\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 34\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 35\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 36\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 37\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 38\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 39\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 40\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 41\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 42\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 43\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 44\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 45\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 46\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 47\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 48\t Discriminator loss: 1.0064\t Generator loss: 0.6931\n",
      "Iterations: 49\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 50\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 51\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 52\t Discriminator loss: 1.0064\t Generator loss: 0.6931\n",
      "Iterations: 53\t Discriminator loss: 1.0064\t Generator loss: 0.6931\n",
      "Iterations: 54\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 55\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 56\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 57\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 58\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 59\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 60\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 61\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 62\t Discriminator loss: 1.0064\t Generator loss: 0.6931\n",
      "Iterations: 63\t Discriminator loss: 1.0065\t Generator loss: 0.6931\n",
      "Iterations: 64\t Discriminator loss: 1.0064\t Generator loss: 0.6931\n",
      "Iterations: 65\t Discriminator loss: 1.0064\t Generator loss: 0.6931\n",
      "Iterations: 66\t Discriminator loss: 1.0064\t Generator loss: 0.6931\n",
      "Iterations: 67\t Discriminator loss: 1.0064\t Generator loss: 0.6931\n",
      "Iterations: 68\t Discriminator loss: 1.0064\t Generator loss: 0.6931\n",
      "Iterations: 69\t Discriminator loss: 1.0064\t Generator loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "# Iterate through epochs\n",
    "fake_saved = []\n",
    "real_saved = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i_batch, sample_batched in enumerate(data_loader):\n",
    "        # Train Discriminator\n",
    "        feed_dict = {X: sample_batched, Z: noise(batch_size, noise_size)}\n",
    "        _, d_error, = session.run([D_opt, D_loss], feed_dict=feed_dict)\n",
    "\n",
    "        # 2. Train Generator\n",
    "        feed_dict = {Z: noise(batch_size, noise_size)}\n",
    "        _, g_error = session.run([G_opt, G_loss], feed_dict=feed_dict)\n",
    "\n",
    "        if i_batch % 100 == 0:\n",
    "            save = session.run(G_sample, feed_dict={Z: noise(batch_size, noise_size)})\n",
    "            fake_saved.append(save)\n",
    "            # pdb.set_trace()\n",
    "            array_out = X.eval(session=session, feed_dict={X: sample_batched})\n",
    "            real_saved.append(array_out)\n",
    "\n",
    "            print(\"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\" % (epoch, d_error, g_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-aacf188ad5ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_saved\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Select a real sample day\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfake_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfake_saved\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Select a synthetic sample day\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'poster'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot real and fake data\n",
    "# real_data = scaler_x.inverse_transform(real_saved)\n",
    "# fake_data = scaler_x.inverse_transform(fake_saved[75][1])\n",
    "real_data = real_saved[7][10] # Select a real sample day\n",
    "fake_data = fake_saved[7][10] # Select a synthetic sample day\n",
    "sns.set_context('poster')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "plt.sca(ax)\n",
    "plt.plot(real_data, label=\"real data\")\n",
    "plt.legend()\n",
    "plt.title('Sample Day Real Data')\n",
    "plt.xlabel('Time (minutes)')\n",
    "plt.ylabel('Normalized Total Active Power (kW)')\n",
    "plt.grid(linewidth=0.75, alpha=0.25)\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_fx = MinMaxScaler()\n",
    "fake_data_fit = scaler_fx.fit_transform(fake_saved[7])\n",
    "\n",
    "sns.set_context('poster')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "plt.sca(ax)\n",
    "plt.plot(real_data, label=\"real data\")\n",
    "plt.plot(fake_data_fit[10], label=\"fake data\")\n",
    "plt.legend()\n",
    "plt.title('Sample Day Real vs Fake Data')\n",
    "plt.xlabel('Time (minutes)')\n",
    "plt.ylabel('Normalized Total Active Power (kW)')\n",
    "plt.grid(linewidth=0.75, alpha=0.25)\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
